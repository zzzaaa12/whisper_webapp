"""
çµ±ä¸€çš„AIæ‘˜è¦æœå‹™æ¨¡çµ„
æ•´åˆæ‰€æœ‰AIæ‘˜è¦ç›¸é—œåŠŸèƒ½ï¼Œé¿å…ä»£ç¢¼é‡è¤‡
"""

import os
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, Callable
import traceback

class SummaryService:
    """çµ±ä¸€çš„æ‘˜è¦æœå‹™é¡åˆ¥"""

    def __init__(self, openai_api_key: Optional[str] = None, config_getter: Optional[Callable] = None):
        """
        åˆå§‹åŒ–æ‘˜è¦æœå‹™

        Args:
            openai_api_key: OpenAI API é‡‘é‘°
            config_getter: é…ç½®ç²å–å‡½æ•¸ï¼Œç”¨æ–¼ç²å–å„ç¨®é…ç½®å€¼
        """
        self.openai_api_key = openai_api_key
        self.config_getter = config_getter or (lambda key, default=None: os.getenv(key, default))
        self.openai = None
        self._init_openai()

    def _init_openai(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯"""
        if not self.openai_api_key:
            self.openai_api_key = self.config_getter("OPENAI_API_KEY")

        if self.openai_api_key and not self.openai:
            try:
                import openai
                self.openai = openai
            except ImportError:
                print("[SUMMARY] Warning: OpenAI library not installed")

    def _get_model_config(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹é…ç½®"""
        max_tokens_str = self.config_getter("OPENAI_MAX_TOKENS", "20000")
        temperature_str = self.config_getter("OPENAI_TEMPERATURE", "0.7")

        return {
            'model': self.config_getter("OPENAI_MODEL", "gpt-4.1-mini"),
            'max_tokens': int(max_tokens_str) if max_tokens_str is not None else 20000,
            'temperature': float(temperature_str) if temperature_str is not None else 0.7
        }

    def _create_prompt(self, subtitle_content: str, prompt_type: str = "structured") -> str:
        """
        å‰µå»ºæ‘˜è¦prompt

        Args:
            subtitle_content: å­—å¹•å…§å®¹
            prompt_type: prompté¡å‹ ('simple', 'structured', 'detailed')
        """
        # çµ±ä¸€ä½¿ç”¨æ–°çš„promptæ ¼å¼
        prompt = f"è«‹æ•´ç†ä¸€ä¸‹é€™å€‹youtubeå°è©±ç´€éŒ„ï¼Œæ¢åˆ—å‡ºæ¯ä¸€å€‹é …ç›®èˆ‡å…§å®¹ï¼Œæ”¾åœ¨æœ€å‰é¢çš„æ®µè½ï¼šã€å½±ç‰‡å…§å®¹æ‘˜è¦ã€‘\n\n{subtitle_content}"

        return prompt

    def _add_header(self, summary: str, header_info: Dict[str, Any]) -> str:
        """
        ç‚ºæ‘˜è¦æ·»åŠ æª”æ¡ˆè³‡è¨Šheader

        Args:
            summary: åŸå§‹æ‘˜è¦å…§å®¹
            header_info: headerè³‡è¨Šå­—å…¸
        """
        header_lines = []

        # æª”æ¡ˆè³‡è¨Š
        if 'filename' in header_info:
            header_lines.append(f"ğŸ“ æª”æ¡ˆï¼š{header_info['filename']}")

        # å½±ç‰‡è³‡è¨Š
        if 'title' in header_info:
            header_lines.append(f"ğŸ¬ æ¨™é¡Œï¼š{header_info['title']}")
        if 'uploader' in header_info:
            header_lines.append(f"ğŸ“º é »é“ï¼š{header_info['uploader']}")
        if 'url' in header_info:
            header_lines.append(f"ğŸ”— ç¶²å€ï¼š{header_info['url']}")

        # è™•ç†æ™‚é–“
        header_lines.append(f"â° è™•ç†æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # ç”Ÿæˆå®Œæ•´header
        if header_lines:
            header = "\n".join(header_lines) + f"\n{'='*50}\n\n"
            return header + summary

        return summary

    def generate_summary(self,
                        subtitle_content: str,
                        prompt_type: str = "structured",
                        header_info: Optional[Dict[str, Any]] = None,
                        progress_callback: Optional[Callable] = None,
                        log_callback: Optional[Callable] = None) -> tuple[bool, str]:
        """
        ç”Ÿæˆæ‘˜è¦

        Args:
            subtitle_content: å­—å¹•å…§å®¹
            prompt_type: prompté¡å‹
            header_info: headerè³‡è¨Šï¼ˆå¯é¸ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼‰
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼‰

        Returns:
            tuple: (æˆåŠŸæ¨™èªŒ, æ‘˜è¦å…§å®¹æˆ–éŒ¯èª¤ä¿¡æ¯)
        """
        if not self.openai_api_key:
            error_msg = "âŒ OpenAI API key æœªè¨­å®šï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦"
            if log_callback:
                log_callback(error_msg, 'error')
            return False, error_msg

        if not subtitle_content or not subtitle_content.strip():
            error_msg = "âŒ å­—å¹•å…§å®¹ç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦"
            if log_callback:
                log_callback(error_msg, 'error')
            return False, error_msg

        try:
            if log_callback:
                log_callback("â–¶ï¸ é–‹å§‹ç”Ÿæˆ AI æ‘˜è¦...", 'info')

            if progress_callback:
                progress_callback(90)

            # åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯
            if not self.openai:
                self._init_openai()

            if not self.openai:
                error_msg = "âŒ OpenAI æ¨¡çµ„è¼‰å…¥å¤±æ•—"
                if log_callback:
                    log_callback(error_msg, 'error')
                return False, error_msg

            client = self.openai.OpenAI(api_key=self.openai_api_key)

            # å‰µå»ºprompt
            prompt = self._create_prompt(subtitle_content, prompt_type)

            # ç²å–æ¨¡å‹é…ç½®
            model_config = self._get_model_config()

            if log_callback:
                log_callback(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{model_config['model']}", 'info')

            # èª¿ç”¨OpenAI API
            response = client.chat.completions.create(
                model=model_config['model'],
                messages=[
                    {"role": "system", "content": "You are a helpful assistant. ç”¨å°ç£ç”¨èªèˆ‡æ­£é«”ä¸­æ–‡å›ç­”"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=model_config['max_tokens'],
                temperature=model_config['temperature']
            )

            # æå–æ‘˜è¦å…§å®¹
            summary_content = response.choices[0].message.content
            summary = summary_content.strip() if summary_content else ""

            if not summary:
                error_msg = "âš ï¸ AI æœªå›å‚³æœ‰æ•ˆæ‘˜è¦å…§å®¹"
                if log_callback:
                    log_callback(error_msg, 'warning')
                return False, error_msg

            # æ·»åŠ headerï¼ˆå¦‚æœæä¾›ï¼‰
            if header_info:
                summary = self._add_header(summary, header_info)

            if progress_callback:
                progress_callback(100)

            if log_callback:
                log_callback("âœ… AI æ‘˜è¦ç”Ÿæˆå®Œæˆ", 'success')

            return True, summary

        except Exception as e:
            error_msg = f"âŒ AI æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}"
            if log_callback:
                log_callback(error_msg, 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
            return False, error_msg

    def save_summary(self,
                    summary: str,
                    save_path: Path,
                    log_callback: Optional[Callable] = None) -> bool:
        """
        å„²å­˜æ‘˜è¦åˆ°æª”æ¡ˆ

        Args:
            summary: æ‘˜è¦å…§å®¹
            save_path: å„²å­˜è·¯å¾‘
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼‰

        Returns:
            bool: å„²å­˜æˆåŠŸæ¨™èªŒ
        """
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # å¯«å…¥æª”æ¡ˆ
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(summary)

            if log_callback:
                log_callback(f"ğŸ“„ æ‘˜è¦å·²å„²å­˜è‡³: {save_path}", 'info')

            return True

        except Exception as e:
            error_msg = f"âŒ æ‘˜è¦å„²å­˜å¤±æ•—: {str(e)}"
            if log_callback:
                log_callback(error_msg, 'error')
            return False

    def generate_and_save_summary(self,
                                 subtitle_content: str,
                                 save_path: Path,
                                 prompt_type: str = "structured",
                                 header_info: Optional[Dict[str, Any]] = None,
                                 progress_callback: Optional[Callable] = None,
                                 log_callback: Optional[Callable] = None,
                                 telegram_callback: Optional[Callable] = None) -> tuple[bool, str]:
        """
        ç”Ÿæˆä¸¦å„²å­˜æ‘˜è¦ï¼ˆä¸€ç«™å¼æœå‹™ï¼‰

        Args:
            subtitle_content: å­—å¹•å…§å®¹
            save_path: å„²å­˜è·¯å¾‘
            prompt_type: prompté¡å‹
            header_info: headerè³‡è¨Š
            progress_callback: é€²åº¦å›èª¿
            log_callback: æ—¥èªŒå›èª¿
            telegram_callback: Telegramé€šçŸ¥å›èª¿

        Returns:
            tuple: (æˆåŠŸæ¨™èªŒ, æ‘˜è¦å…§å®¹æˆ–éŒ¯èª¤ä¿¡æ¯)
        """
        # ç”Ÿæˆæ‘˜è¦
        success, result = self.generate_summary(
            subtitle_content=subtitle_content,
            prompt_type=prompt_type,
            header_info=header_info,
            progress_callback=progress_callback,
            log_callback=log_callback
        )

        if not success:
            return False, result

        summary = result

        # å„²å­˜æ‘˜è¦
        if not self.save_summary(summary, save_path, log_callback):
            return False, "æ‘˜è¦ç”ŸæˆæˆåŠŸä½†å„²å­˜å¤±æ•—"

        # ç™¼é€æ—¥èªŒ
        if log_callback:
            log_callback(f"---æ‘˜è¦å…§å®¹---\n{summary}", 'info')

        # ç™¼é€Telegramé€šçŸ¥ï¼ˆå¦‚æœæä¾›ï¼‰
        if telegram_callback and header_info:
            try:
                # æå–åŸå§‹æ‘˜è¦ï¼ˆå»é™¤headerï¼‰
                lines = summary.split('\n')
                summary_start = 0
                for i, line in enumerate(lines):
                    if '=' in line and len(line) > 20:  # æ‰¾åˆ°åˆ†éš”ç·š
                        summary_start = i + 2
                        break

                clean_summary = '\n'.join(lines[summary_start:]) if summary_start > 0 else summary

                # æ§‹å»ºTelegramæ¶ˆæ¯
                if 'title' in header_info:
                    # å½±ç‰‡é¡å‹
                    tg_message = (
                        f"âœ… *æ‘˜è¦å®Œæˆ:*\n\n"
                        f"ğŸ¬ *æ¨™é¡Œ:* `{header_info.get('title', 'N/A')}`\n"
                        f"ğŸ“º *é »é“:* `{header_info.get('uploader', 'N/A')}`\n\n"
                        f"ğŸ“ *å®Œæ•´æ‘˜è¦:*\n`{clean_summary[:1000]}{'...' if len(clean_summary) > 1000 else ''}`"
                    )
                else:
                    # æª”æ¡ˆé¡å‹
                    tg_message = (
                        f"âœ… *æ‘˜è¦å®Œæˆ:*\n\n"
                        f"ğŸ“ *æª”æ¡ˆ:* `{header_info.get('filename', 'N/A')}`\n\n"
                        f"ğŸ“ *å®Œæ•´æ‘˜è¦:*\n`{clean_summary[:1000]}{'...' if len(clean_summary) > 1000 else ''}`"
                    )

                telegram_callback(tg_message)

            except Exception as e:
                if log_callback:
                    log_callback(f"âš ï¸ Telegramé€šçŸ¥ç™¼é€å¤±æ•—: {e}", 'warning')

        return True, summary


# å…¨åŸŸæ‘˜è¦æœå‹™å¯¦ä¾‹
_summary_service_instance = None

def get_summary_service(openai_api_key: Optional[str] = None, config_getter: Optional[Callable] = None) -> SummaryService:
    """
    ç²å–å…¨åŸŸæ‘˜è¦æœå‹™å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰

    Args:
        openai_api_key: OpenAI API é‡‘é‘°
        config_getter: é…ç½®ç²å–å‡½æ•¸

    Returns:
        SummaryService: æ‘˜è¦æœå‹™å¯¦ä¾‹
    """
    global _summary_service_instance

    if _summary_service_instance is None:
        _summary_service_instance = SummaryService(openai_api_key, config_getter)
    elif openai_api_key:
        # æ›´æ–°APIé‡‘é‘°
        _summary_service_instance.openai_api_key = openai_api_key
        _summary_service_instance._init_openai()

    return _summary_service_instance

def reset_summary_service():
    """é‡ç½®å…¨åŸŸæ‘˜è¦æœå‹™å¯¦ä¾‹"""
    global _summary_service_instance
    _summary_service_instance = None