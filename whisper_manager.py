"""
çµ±ä¸€Whisperæ¨¡å‹ç®¡ç†å™¨ - æ•´åˆæ‰€æœ‰æ¨¡å‹è¼‰å…¥å’Œè½‰éŒ„é‚è¼¯
"""

import torch
import traceback
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any, Callable
from src.config import get_config
from src.utils.time_formatter import get_timestamp


class WhisperModelManager:
    """çµ±ä¸€Whisperæ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, model_name: str = "asadfgglie/faster-whisper-large-v3-zh-TW"):
        self.model_name = model_name
        self.model = None
        self.device = "cpu"
        self.compute_type = "int8"
        self.is_loaded = False

    def load_model(self, prefer_cuda: bool = True, log_callback: Optional[Callable] = None) -> bool:
        """
        çµ±ä¸€æ¨¡å‹è¼‰å…¥å‡½æ•¸

        Args:
            prefer_cuda: æ˜¯å¦å„ªå…ˆä½¿ç”¨CUDA
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸

        Returns:
            bool: è¼‰å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            import faster_whisper

            if log_callback:
                log_callback("ğŸ”„ è¼‰å…¥ Whisper æ¨¡å‹...", 'info')

            # é‡ç½®ç‹€æ…‹
            self.model = None
            self.is_loaded = False

            # è¨­å®šåˆå§‹è¨­å‚™é…ç½®
            self.device = "cpu"
            self.compute_type = "int8"

            # å˜—è©¦ä½¿ç”¨ CUDAï¼ˆå¦‚æœåå¥½ä¸”å¯ç”¨ï¼‰
            if prefer_cuda and torch.cuda.is_available():
                try:
                    # æ¸¬è©¦ CUDA æ˜¯å¦çœŸçš„å¯ä»¥å·¥ä½œ
                    test_tensor = torch.zeros(1, device="cuda")
                    del test_tensor

                    self.device = "cuda"
                    self.compute_type = "float16"

                    if log_callback:
                        log_callback("âœ… CUDA æ¸¬è©¦æˆåŠŸï¼Œä½¿ç”¨ GPU åŠ é€Ÿ", 'success')

                except Exception as cuda_error:
                    if log_callback:
                        log_callback(f"âš ï¸ CUDA æ¸¬è©¦å¤±æ•—ï¼š{cuda_error}ï¼Œå›é€€åˆ° CPU", 'warning')

                    self.device = "cpu"
                    self.compute_type = "int8"

            # è¼‰å…¥æ¨¡å‹
            if log_callback:
                log_callback(f"ğŸ”„ è¼‰å…¥æ¨¡å‹ (è¨­å‚™: {self.device}, è¨ˆç®—é¡å‹: {self.compute_type})", 'info')

            self.model = faster_whisper.WhisperModel(
                self.model_name,
                device=self.device,
                compute_type=self.compute_type
            )

            self.is_loaded = True

            if log_callback:
                log_callback(f"âœ… Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ ({self.device})", 'success')

            return True

        except Exception as e:
            error_msg = f"âŒ Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}"
            if log_callback:
                log_callback(error_msg, 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')

            self.is_loaded = False
            return False

    def transcribe_with_fallback(
        self,
        audio_file: str,
        log_callback: Optional[Callable] = None,
        **transcribe_kwargs
    ) -> Tuple[bool, Optional[List]]:
        """
        å¸¶å›é€€æ©Ÿåˆ¶çš„è½‰éŒ„å‡½æ•¸

        Args:
            audio_file: éŸ³æª”è·¯å¾‘
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸
            **transcribe_kwargs: è½‰éŒ„åƒæ•¸

        Returns:
            Tuple[bool, Optional[List]]: (æˆåŠŸç‹€æ…‹, ç‰‡æ®µåˆ—è¡¨)
        """
        if not self.is_loaded or not self.model:
            if log_callback:
                log_callback("âŒ æ¨¡å‹æœªè¼‰å…¥", 'error')
            return False, None

        # è¨­å®šé è¨­è½‰éŒ„åƒæ•¸
        default_params = {
            'beam_size': 1,
            'language': "zh",
            'vad_filter': True
        }
        default_params.update(transcribe_kwargs)

        try:
            if log_callback:
                log_callback("ğŸ¯ é–‹å§‹è½‰éŒ„éŸ³æª”...", 'info')
                log_callback("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è½‰éŒ„...", 'info')

            # ç¬¬ä¸€æ¬¡å˜—è©¦è½‰éŒ„
            segments, _ = self.model.transcribe(str(audio_file), **default_params)

            if log_callback:
                log_callback("ğŸ”„ è½‰éŒ„é€²è¡Œä¸­ï¼Œæ­£åœ¨è™•ç†ç‰‡æ®µ...", 'info')

            # è½‰æ›ç‚ºåˆ—è¡¨
            segments_list = list(segments)

            if log_callback:
                log_callback(f"âœ… è½‰éŒ„å®Œæˆï¼Œå…± {len(segments_list)} å€‹ç‰‡æ®µ", 'success')

            return True, segments_list

        except RuntimeError as e:
            # æª¢æŸ¥æ˜¯å¦ç‚ºCUDAç›¸é—œéŒ¯èª¤
            if "cublas" in str(e).lower() or "cuda" in str(e).lower():
                if log_callback:
                    log_callback("âš ï¸ CUDA éŒ¯èª¤ï¼Œå˜—è©¦ä½¿ç”¨ CPU é‡æ–°è½‰éŒ„...", 'warning')

                # å˜—è©¦CPUå›é€€
                return self._cpu_fallback_transcribe(audio_file, log_callback, **default_params)
            else:
                if log_callback:
                    log_callback(f"âŒ è½‰éŒ„å¤±æ•—: {e}", 'error')
                    log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
                return False, None

        except Exception as e:
            if log_callback:
                log_callback(f"âŒ è½‰éŒ„å¤±æ•—: {e}", 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
            return False, None

    def _cpu_fallback_transcribe(
        self,
        audio_file: str,
        log_callback: Optional[Callable] = None,
        **transcribe_kwargs
    ) -> Tuple[bool, Optional[List]]:
        """CPUå›é€€è½‰éŒ„"""
        try:
            import faster_whisper

            if log_callback:
                log_callback("ğŸ”„ é‡æ–°è¼‰å…¥ CPU æ¨¡å‹...", 'info')

            # é‡æ–°è¼‰å…¥CPUæ¨¡å‹
            self.model = faster_whisper.WhisperModel(
                self.model_name,
                device="cpu",
                compute_type="int8"
            )

            self.device = "cpu"
            self.compute_type = "int8"

            # é‡æ–°å˜—è©¦è½‰éŒ„
            segments, _ = self.model.transcribe(str(audio_file), **transcribe_kwargs)

            if log_callback:
                log_callback("ğŸ”„ CPU è½‰éŒ„é€²è¡Œä¸­...", 'info')

            segments_list = list(segments)

            if log_callback:
                log_callback(f"âœ… CPU è½‰éŒ„å®Œæˆï¼Œå…± {len(segments_list)} å€‹ç‰‡æ®µ", 'success')

            return True, segments_list

        except Exception as cpu_error:
            if log_callback:
                log_callback(f"âŒ CPU è½‰éŒ„ä¹Ÿå¤±æ•—: {cpu_error}", 'error')
            return False, None

    def get_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ç‹€æ…‹"""
        return {
            'is_loaded': self.is_loaded,
            'device': self.device,
            'device_name': torch.cuda.get_device_name(0) if self.device == 'cuda' else 'CPU',
            'compute_type': self.compute_type,
            'cuda_available': self.device == 'cuda',
            'model_name': self.model_name,
            'last_updated': get_timestamp()
        }

    def unload_model(self):
        """å¸è¼‰æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”"""
        if self.model:
            del self.model
            self.model = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.is_loaded = False


# å…¨åŸŸæ¨¡å‹ç®¡ç†å™¨å¯¦ä¾‹
whisper_manager = WhisperModelManager()


def get_whisper_manager() -> WhisperModelManager:
    """ç²å–å…¨åŸŸWhisperæ¨¡å‹ç®¡ç†å™¨"""
    return whisper_manager


def transcribe_audio(
    audio_file: str,
    log_callback: Optional[Callable] = None,
    auto_load: bool = True,
    **kwargs
) -> Tuple[bool, Optional[List]]:
    """
    ä¾¿æ·éŸ³æª”è½‰éŒ„å‡½æ•¸

    Args:
        audio_file: éŸ³æª”è·¯å¾‘
        log_callback: æ—¥èªŒå›èª¿å‡½æ•¸
        auto_load: æ˜¯å¦è‡ªå‹•è¼‰å…¥æ¨¡å‹
        **kwargs: å…¶ä»–è½‰éŒ„åƒæ•¸

    Returns:
        Tuple[bool, Optional[List]]: (æˆåŠŸç‹€æ…‹, ç‰‡æ®µåˆ—è¡¨)
    """
    manager = get_whisper_manager()

    # å¦‚æœæ¨¡å‹æœªè¼‰å…¥ä¸”å…è¨±è‡ªå‹•è¼‰å…¥
    if not manager.is_loaded and auto_load:
        if not manager.load_model(log_callback=log_callback):
            return False, None

    return manager.transcribe_with_fallback(audio_file, log_callback, **kwargs)