"""
çµ±ä¸€çš„AIæ‘˜è¦æœå‹™æ¨¡çµ„
æ•´åˆæ‰€æœ‰AIæ‘˜è¦ç›¸é—œåŠŸèƒ½ï¼Œé¿å…ä»£ç¢¼é‡è¤‡
"""

from typing import Optional, Dict, Any, Callable
from pathlib import Path
from datetime import datetime # Added for datetime usage
from src.config import get_config
from src.utils.logger_manager import get_logger_manager

class SummaryService:
    """çµ±ä¸€çš„æ‘˜è¦æœå‹™é¡åˆ¥ - æ”¯æ´å¤š AI æä¾›å•†"""

    def __init__(self, openai_api_key: Optional[str] = None, ai_provider: Optional[str] = None):
        """
        åˆå§‹åŒ–æ‘˜è¦æœå‹™

        Args:
            openai_api_key: OpenAI API é‡‘é‘°ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            ai_provider: AI æä¾›å•†åç¨± (openai, claude, ollama, groq ç­‰)
        """
        self.openai = None
        self.logger_manager = get_logger_manager()

        # æ±ºå®šä½¿ç”¨çš„ AI æä¾›å•†
        self.ai_provider = ai_provider or get_config("AI_PROVIDER", "openai")

        # å‘å¾Œå…¼å®¹ï¼šå¦‚æœç›´æ¥å‚³å…¥ openai_api_keyï¼Œå‰‡ä½¿ç”¨ openai æä¾›å•†
        if openai_api_key:
            self.ai_provider = "openai"
            self._legacy_api_key = openai_api_key
        else:
            self._legacy_api_key = None

        # ç•¶å‰æä¾›å•†é…ç½®
        self.current_provider_config = None
        self._fallback_tried = []  # è¨˜éŒ„å·²å˜—è©¦çš„æä¾›å•†

        self._init_ai_client()

    def _get_provider_config(self, provider_name: str) -> Optional[Dict[str, Any]]:
        """ç²å–æŒ‡å®š AI æä¾›å•†çš„é…ç½®"""
        try:
            # å˜—è©¦å¾æ–°ç‰ˆé…ç½®ä¸­ç²å–
            providers_config = get_config("AI_PROVIDERS", {})
            if isinstance(providers_config, dict) and provider_name in providers_config:
                config = providers_config[provider_name].copy()

                # è™•ç†ç‰¹æ®Šæƒ…æ³ï¼šå¦‚æœä½¿ç”¨ legacy API key
                if provider_name == "openai" and self._legacy_api_key:
                    config["api_key"] = self._legacy_api_key

                # æª¢æŸ¥ API key æ˜¯å¦åŒ…å« "é‡‘é‘°" - å¦‚æœåŒ…å«å‰‡è¡¨ç¤ºæœªè¨­ç½®æœ‰æ•ˆé‡‘é‘°
                api_key = config.get("api_key", "")
                if "é‡‘é‘°" in str(api_key):
                    self.logger_manager.debug(f"Provider {provider_name} API key contains placeholder, skipping", "ai_summary")
                    return None
                elif api_key == "":
                    self.logger_manager.debug(f"Provider {provider_name} API key is empty, skipping", "ai_summary")
                    return None

                return config

            # å‘å¾Œå…¼å®¹ï¼šå¾èˆŠç‰ˆé…ç½®æ§‹å»º openai é…ç½®
            if provider_name == "openai":
                api_key = self._legacy_api_key or get_config("OPENAI_API_KEY")
                if api_key:
                    # æª¢æŸ¥èˆŠç‰ˆ API key æ˜¯å¦åŒ…å« "é‡‘é‘°"
                    if "é‡‘é‘°" in str(api_key):
                        self.logger_manager.debug("Legacy OpenAI API key contains placeholder, skipping", "ai_summary")
                        return None

                    return {
                        "api_key": api_key,
                        "base_url": "https://api.openai.com/v1",
                        "model": get_config("OPENAI_MODEL", "gpt-4o-mini"),
                        "max_tokens": int(get_config("OPENAI_MAX_TOKENS", "10000") or "10000"),
                        "temperature": float(get_config("OPENAI_TEMPERATURE", "0.7") or "0.7")
                    }

            return None

        except Exception as e:
            self.logger_manager.error(f"Error getting provider config for {provider_name}: {e}", "ai_summary")
            return None

    def _init_ai_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ¶ç«¯"""
        self.current_provider_config = self._get_provider_config(self.ai_provider or "openai")

        if not self.current_provider_config:
            self.logger_manager.warning(f"No valid config found for AI provider '{self.ai_provider}'", "ai_summary")

            # å˜—è©¦å®¹éŒ¯åˆ‡æ›åˆ°å…¶ä»–å¯ç”¨çš„æä¾›å•†
            if self._try_fallback_provider():
                self.logger_manager.info(f"Successfully switched to fallback provider: {self.ai_provider}", "ai_summary")
            else:
                self.logger_manager.warning("No valid AI providers available", "ai_summary")
                return

        # å†æ¬¡æª¢æŸ¥é…ç½®æ˜¯å¦æœ‰æ•ˆï¼ˆå¯èƒ½å·²ç¶“é€šéå®¹éŒ¯åˆ‡æ›æ›´æ–°ï¼‰
        if not self.current_provider_config or not self.current_provider_config.get("api_key"):
            self.logger_manager.warning(f"No valid API key found for AI provider '{self.ai_provider}'", "ai_summary")
            return

        if not self.openai:
            try:
                import openai
                self.openai = openai
            except ImportError:
                self.logger_manager.warning("OpenAI library not installed", "ai_summary")

    def _get_model_config(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æä¾›å•†çš„æ¨¡å‹é…ç½®"""
        if not self.current_provider_config:
            # å‘å¾Œå…¼å®¹çš„é è¨­é…ç½®
            return {
                'model': get_config("OPENAI_MODEL", "gpt-4o-mini"),
                'max_tokens': int(get_config("OPENAI_MAX_TOKENS", "10000") or "10000"),
                'temperature': float(get_config("OPENAI_TEMPERATURE", "0.7") or "0.7")
            }

        return {
            'model': self.current_provider_config.get('model', 'gpt-4o-mini'),
            'max_tokens': self.current_provider_config.get('max_tokens', 10000),
            'temperature': self.current_provider_config.get('temperature', 0.7)
        }

    def _try_fallback_provider(self, log_callback: Optional[Callable] = None) -> bool:
        """å˜—è©¦å®¹éŒ¯åˆ‡æ›åˆ°ä¸‹ä¸€å€‹å¯ç”¨çš„æä¾›å•†"""
        fallback_enabled = get_config("AI_FALLBACK_ENABLED", True)
        if not fallback_enabled:
            return False

        fallback_order = get_config("AI_FALLBACK_ORDER", ["openai", "claude", "groq", "ollama"])
        if not isinstance(fallback_order, list):
            return False

        # è¨˜éŒ„ç•¶å‰å¤±æ•—çš„æä¾›å•†
        if self.ai_provider not in self._fallback_tried:
            self._fallback_tried.append(self.ai_provider)

        # æ‰¾åˆ°ä¸‹ä¸€å€‹å¯ç”¨çš„æä¾›å•†
        for provider in fallback_order:
            if provider not in self._fallback_tried:
                provider_config = self._get_provider_config(provider)
                if provider_config and provider_config.get("api_key"):
                    if log_callback:
                        log_callback(f"ğŸ”„ åˆ‡æ›åˆ° AI æä¾›å•†: {provider}", 'info')

                    self.ai_provider = provider
                    self.current_provider_config = provider_config
                    return True

        return False

    def _call_ai_api(self, prompt: str, model_config: Dict[str, Any], log_callback: Optional[Callable]) -> Any:
        if not self.openai:
            self._init_ai_client()

        if not self.openai:
            raise RuntimeError("OpenAI module failed to load.")

        # å‹•æ…‹å‰µå»ºå®¢æˆ¶ç«¯ï¼ˆæ”¯æ´ä¸åŒçš„ base_urlï¼‰
        client_kwargs = {"api_key": self.current_provider_config["api_key"]}

        # å¦‚æœæœ‰è‡ªå®šç¾©çš„ base_urlï¼Œå‰‡è¨­å®š
        if "base_url" in self.current_provider_config:
            base_url = self.current_provider_config["base_url"]
            if base_url != "https://api.openai.com/v1":  # éé è¨­çš„æ‰è¨­å®š
                client_kwargs["base_url"] = base_url

        client = self.openai.OpenAI(**client_kwargs)

        if log_callback:
            log_callback(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{model_config['model']} (æä¾›å•†: {self.ai_provider})", 'info')

        # èª¿ç”¨ AI API
        response = client.chat.completions.create(
            model=model_config['model'],
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å½±ç‰‡å…§å®¹æ‘˜è¦å°ˆå®¶ï¼Œæ“…é•·å¾å­—å¹•ä¸­æå–é‡é»ä¸¦æ•´ç†æˆçµæ§‹åŒ–æ‘˜è¦ã€‚è«‹ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›ç­”ï¼Œä¿æŒç°¡æ½”ç²¾æº–ã€‚å­—å¹•æ˜¯é€éèªéŸ³è½‰æ–‡å­—çš„æ–¹å¼ç”¢å‡ºï¼Œå¦‚æœæ–‡å­—æˆ–è©èªæœ‰æ˜é¡¯éŒ¯èª¤ï¼Œå¯å…ˆä¿®æ­£å†å›ç­”ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=model_config['max_tokens'],
            temperature=model_config['temperature']
        )
        return response

    def _create_prompt(self, subtitle_content: str, prompt_type: str = "structured") -> str:
        """
        å‰µå»ºæ‘˜è¦prompt

        Args:
            subtitle_content: å­—å¹•å…§å®¹
            prompt_type: prompté¡å‹ ('simple', 'structured', 'detailed')
        """
        if prompt_type == "simple":
            # ç°¡å–®æ¨¡å¼ - ä¿æŒåŸæœ‰æ ¼å¼
            prompt = f"è«‹æ•´ç†ä¸€ä¸‹é€™å€‹youtubeå°è©±ç´€éŒ„ï¼Œæ¢åˆ—å‡ºæ¯ä¸€å€‹é …ç›®èˆ‡å…§å®¹\n\n{subtitle_content}"
        elif prompt_type == "detailed":
            # è©³ç´°æ¨¡å¼ - æ›´è±å¯Œçš„ Markdown æ ¼å¼
            prompt = f"""è«‹å°‡ä»¥ä¸‹å½±ç‰‡å…§å®¹æ•´ç†æˆè©³ç´°çš„çµæ§‹åŒ–æ‘˜è¦ï¼Œä½¿ç”¨ Markdown æ ¼å¼ï¼š

## ğŸ¯ å½±ç‰‡å…§å®¹æ‘˜è¦

è«‹åˆ†æä»¥ä¸‹å…§å®¹ä¸¦æŒ‰ç…§ä¸‹åˆ—çµæ§‹æ•´ç†ï¼š

### ğŸ“‹ ä¸»è¦è­°é¡Œ
- åˆ—å‡ºå½±ç‰‡è¨è«–çš„æ ¸å¿ƒä¸»é¡Œ

### ğŸ’¡ é‡é»å…§å®¹
- é‡è¦è§€é»å’Œè¦‹è§£
- é—œéµæ•¸æ“šæˆ–äº‹å¯¦
- å€¼å¾—æ³¨æ„çš„è«–è¿°

### ğŸ” è©³ç´°åˆ†æ
æ ¹æ“šå…§å®¹æ·±åº¦åˆ†æ®µèªªæ˜å„å€‹é‡é»

### ğŸ“Œ çµè«–èˆ‡è¦é»
- ç¸½çµé‡è¦çµè«–
- å¯è¡Œå‹•çš„å»ºè­°æˆ–å•Ÿç™¼

---

**å½±ç‰‡åŸå§‹å…§å®¹ï¼š**
{subtitle_content}"""

        else:
            # çµæ§‹åŒ–æ¨¡å¼ï¼ˆé è¨­ï¼‰- é‡å° GPT-4.1-mini å„ªåŒ–ï¼Œæ”¯æ´è‡ªå‹•åˆ¤æ–·å½±ç‰‡é¡å‹
            prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„å½±ç‰‡å…§å®¹æ‘˜è¦å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹å­—å¹•å…§å®¹æ•´ç†æˆçµæ§‹åŒ–æ‘˜è¦ã€‚

ã€ç¬¬ä¸€æ­¥ï¼šåˆ¤æ–·å½±ç‰‡é¡å‹ã€‘
è«‹å…ˆåˆ†æå­—å¹•å…§å®¹ï¼Œåˆ¤æ–·é€™æ˜¯å“ªç¨®é¡å‹çš„å½±ç‰‡ï¼š
- æ•™å­¸é¡ï¼ˆæœ‰æ­¥é©Ÿã€æ“ä½œèªªæ˜ã€å®‰è£è¨­å®šç­‰ï¼‰
- è¨ªè«‡é¡ï¼ˆå°è©±ã€è§€é»äº¤æµã€äººç‰©æ•…äº‹ï¼‰
- æ–°èé¡ï¼ˆäº‹ä»¶å ±å°ã€æ™‚äº‹åˆ†æï¼‰
- è©•è«–é¡ï¼ˆç”¢å“è©•æ¸¬ã€è§€é»è©•è«–ï¼‰
- å…¶ä»–ï¼ˆå¨›æ¨‚ã€Vlog ç­‰ï¼‰

ã€ç¬¬äºŒæ­¥ï¼šæ ¹æ“šé¡å‹é¸æ“‡æ ¼å¼ã€‘

â–¼ å¦‚æœæ˜¯ã€æ•™å­¸é¡ã€‘å½±ç‰‡ï¼Œä½¿ç”¨æ­¤æ ¼å¼ï¼š

## ğŸ¯ æ ¸å¿ƒä¸»é¡Œ
- é€™éƒ¨æ•™å­¸è¦æ•™ä»€éº¼ï¼ˆ1-2 å¥è©±ï¼‰

## ğŸ“‹ å‰ç½®çŸ¥è­˜
- éœ€è¦å…ˆäº†è§£çš„æ¦‚å¿µæˆ–å·¥å…·ï¼ˆè‹¥ç„¡å‰‡å¯«ã€Œç„¡ç‰¹æ®Šè¦æ±‚ã€ï¼‰

## ğŸ“ æ­¥é©Ÿæ•™å­¸
1. **æ­¥é©Ÿæ¨™é¡Œ**
   - å…·é«”æ“ä½œèªªæ˜
   - æ³¨æ„äº‹é …
2. **æ­¥é©Ÿæ¨™é¡Œ**
   - å…·é«”æ“ä½œèªªæ˜
ï¼ˆä¾å…§å®¹åˆ—å‡ºå®Œæ•´æ­¥é©Ÿï¼‰

## âš ï¸ å¸¸è¦‹éŒ¯èª¤èˆ‡æ³¨æ„äº‹é …
- åˆå­¸è€…å®¹æ˜“è¸©çš„å‘æˆ–é‡è¦æé†’

## ğŸ¯ ç¸½çµ
ç”¨ 50-100 å­—ç¸½çµå­¸ç¿’é‡é»

â–¼ å¦‚æœæ˜¯ã€å…¶ä»–é¡å‹ã€‘å½±ç‰‡ï¼Œä½¿ç”¨æ­¤æ ¼å¼ï¼š

## ğŸ¯ æ ¸å¿ƒä¸»é¡Œ
- ç”¨ 1-3 å€‹è¦é»èªªæ˜å½±ç‰‡ä¸»æ—¨

## ğŸ“ é‡é»æ•´ç†
1. **é‡é»æ¨™é¡Œ**
   - å…·é«”èªªæ˜
2. **é‡é»æ¨™é¡Œ**
   - å…·é«”èªªæ˜
ï¼ˆä¾å…§å®¹é•·åº¦åˆ—å‡º 3-8 å€‹é‡é»ï¼‰

## ğŸ’¬ é—œéµé‡‘å¥
> å¼•ç”¨ 3-10 å¥å½±ç‰‡ä¸­çš„é‡è¦è©±èªæˆ–è§€é»

## ğŸ¯ ç¸½çµ
ç”¨ 50-100 å­—ç¸½çµå½±ç‰‡æ ¸å¿ƒåƒ¹å€¼

ã€è¼¸å‡ºè¦å‰‡ã€‘
1. ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰
2. ä¸è¦è¼¸å‡ºã€Œå½±ç‰‡é¡å‹åˆ¤æ–·ã€ï¼Œç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹
3. åš´æ ¼æŒ‰ç…§å°æ‡‰æ ¼å¼çš„å€å¡Šæ¨™é¡Œè¼¸å‡º
4. æ¯å€‹å€å¡Šéƒ½å¿…é ˆæœ‰å…§å®¹ï¼Œä¸å¯çœç•¥

ã€å­—å¹•å…§å®¹ã€‘
{subtitle_content}"""

        return prompt

        # ===== èˆŠç‰ˆ prompt v2ï¼ˆå‚™ä»½ï¼‰=====
        # prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„å½±ç‰‡å…§å®¹æ‘˜è¦å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹å­—å¹•å…§å®¹æ•´ç†æˆçµæ§‹åŒ–æ‘˜è¦ã€‚
        #
        # ã€è¼¸å‡ºè¦å‰‡ã€‘
        # 1. ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰
        # 2. åš´æ ¼æŒ‰ç…§ä»¥ä¸‹å››å€‹å€å¡Šè¼¸å‡ºï¼Œä¸è¦å¢åŠ æˆ–ä¿®æ”¹æ¨™é¡Œ
        # 3. æ¯å€‹å€å¡Šéƒ½å¿…é ˆæœ‰å…§å®¹ï¼Œä¸å¯çœç•¥
        #
        # ã€è¼¸å‡ºæ ¼å¼ã€‘
        #
        # ## ğŸ¯ æ ¸å¿ƒä¸»é¡Œ
        # - ç”¨ 1-3 å€‹è¦é»èªªæ˜å½±ç‰‡ä¸»æ—¨ï¼ˆæ¯é»ä¸è¶…é 30 å­—ï¼‰
        #
        # ## ğŸ“ é‡é»æ•´ç†
        # 1. **é‡é»æ¨™é¡Œ**
        #    - å…·é«”èªªæ˜ï¼ˆæ¯å€‹é‡é» 2-4 å€‹å­é …ç›®ï¼‰
        # 2. **é‡é»æ¨™é¡Œ**
        #    - å…·é«”èªªæ˜
        # ï¼ˆä¾å…§å®¹é•·åº¦åˆ—å‡º 3-8 å€‹é‡é»ï¼‰
        #
        # ## ğŸ’¬ é—œéµé‡‘å¥
        # > å¼•ç”¨ 3-10 å¥å½±ç‰‡ä¸­çš„é‡è¦è©±èªæˆ–è§€é»
        #
        # ## ğŸ¯ ç¸½çµ
        # ç”¨ 50-100 å­—ç¸½çµå½±ç‰‡æ ¸å¿ƒåƒ¹å€¼
        #
        # ã€å­—å¹•å…§å®¹ã€‘
        # {subtitle_content}"""

    def _add_header(self, summary: str, header_info: Dict[str, Any]) -> str:
        """
        ç‚ºæ‘˜è¦æ·»åŠ æª”æ¡ˆè³‡è¨Šheader

        Args:
            summary: åŸå§‹æ‘˜è¦å…§å®¹
            header_info: headerè³‡è¨Šå­—å…¸
        """
        header_lines = []

        # AI æä¾›å•†è³‡è¨Š
        if self.ai_provider and self.current_provider_config:
            model_name = self.current_provider_config.get('model', 'unknown')
            provider_display = {
                'openai': 'OpenAI',
                'claude': 'Anthropic Claude',
                'gemini': 'Google Gemini',
                'deepseek': 'DeepSeek',
                'ollama': 'Ollama (æœ¬åœ°)',
                'grok': 'xAI Grok'
            }.get(self.ai_provider, self.ai_provider.upper())

            header_lines.append(f"ğŸ¤– AI æ‘˜è¦ï¼š{provider_display} ({model_name})")

        # æª”æ¡ˆè³‡è¨Š
        if 'filename' in header_info:
            header_lines.append(f"ğŸ“ æª”æ¡ˆï¼š{header_info['filename']}")

        # å½±ç‰‡è³‡è¨Š
        if 'title' in header_info and header_info['title']:
            header_lines.append(f"ğŸ¬ æ¨™é¡Œï¼š{header_info['title']}")
        if 'uploader' in header_info and header_info['uploader']:
            header_lines.append(f"ğŸ“º é »é“ï¼š{header_info['uploader']}")
        if 'duration_string' in header_info and header_info['duration_string']:
            header_lines.append(f"â±ï¸ å½±ç‰‡é•·åº¦ï¼š{header_info['duration_string']}")
        if 'url' in header_info and header_info['url']:
            header_lines.append(f"ğŸ”— ç¶²å€ï¼š{header_info['url']}")

        # è™•ç†æ™‚é–“
        header_lines.append(f"â° è™•ç†æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # ç”Ÿæˆå®Œæ•´header
        if header_lines:
            header = "\n".join(header_lines) + f"\n{'='*50}\n\n"
            return header + summary

        return summary

    def generate_summary(self,
                        subtitle_content: str,
                        prompt_type: str = "structured",
                        header_info: Optional[Dict[str, Any]] = None,
                        progress_callback: Optional[Callable] = None,
                        log_callback: Optional[Callable] = None) -> tuple[bool, str]:
        """
        ç”Ÿæˆæ‘˜è¦

        Args:
            subtitle_content: å­—å¹•å…§å®¹
            prompt_type: prompté¡å‹
            header_info: headerè³‡è¨Šï¼ˆå¯é¸ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼‰
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼‰

        Returns:
            tuple: (æˆåŠŸæ¨™èªŒ, æ‘˜è¦å…§å®¹æˆ–éŒ¯èª¤ä¿¡æ¯)
        """
        if not self.current_provider_config:
            error_msg = "âŒ ç•¶å‰AIæä¾›å•†é…ç½®æœªåˆå§‹åŒ–ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦"
            if log_callback:
                log_callback(error_msg, 'error')
            return False, error_msg

        if not subtitle_content or not subtitle_content.strip():
            error_msg = "âŒ å­—å¹•å…§å®¹ç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦"
            if log_callback:
                log_callback(error_msg, 'error')
            return False, error_msg

        # æœ€å¤šå˜—è©¦ 3 æ¬¡ï¼ˆåŒ…å«å®¹éŒ¯åˆ‡æ›ï¼‰
        max_attempts = 3
        attempt = 0

        while attempt < max_attempts:
            try:
                attempt += 1

                if log_callback:
                    provider_name = (self.ai_provider or "openai").upper()
                    log_callback(f"â–¶ï¸ é–‹å§‹ç”Ÿæˆ AI æ‘˜è¦ ({provider_name})...", 'info')

                if progress_callback:
                    progress_callback(90)

                # å‰µå»ºprompt
                prompt = self._create_prompt(subtitle_content, prompt_type)

                # ç²å–æ¨¡å‹é…ç½®
                model_config = self._get_model_config()

                response = self._call_ai_api(prompt, model_config, log_callback)

                # æå–æ‘˜è¦å…§å®¹
                summary_content = response.choices[0].message.content
                summary = summary_content.strip() if summary_content else ""

                if not summary:
                    error_msg = "âš ï¸ AI æœªå›å‚³æœ‰æ•ˆæ‘˜è¦å…§å®¹"
                    if log_callback:
                        log_callback(error_msg, 'warning')
                    return False, error_msg

                # æ·»åŠ headerï¼ˆå¦‚æœæä¾›ï¼‰æˆ–è‡³å°‘æ·»åŠ  AI æä¾›å•†ä¿¡æ¯
                if header_info:
                    summary = self._add_header(summary, header_info)
                else:
                    # æ²’æœ‰å®Œæ•´ header æ™‚ï¼Œè‡³å°‘æ·»åŠ  AI æä¾›å•†ä¿¡æ¯
                    if self.ai_provider and self.current_provider_config:
                        from datetime import datetime

                        model_name = self.current_provider_config.get('model', 'unknown')
                        provider_display = {
                            'openai': 'OpenAI',
                            'claude': 'Anthropic Claude',
                            'gemini': 'Google Gemini',
                            'deepseek': 'DeepSeek',
                            'ollama': 'Ollama (æœ¬åœ°)',
                            'grok': 'xAI Grok'
                        }.get(self.ai_provider, self.ai_provider.upper())

                        ai_header = (
                            f"ğŸ¤– AI æ‘˜è¦ï¼š{provider_display} ({model_name})\n"
                            f"â° è™•ç†æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                            f"{'='*50}\n\n"
                        )
                        summary = ai_header + summary

                if progress_callback:
                    progress_callback(100)

                if log_callback:
                    log_callback(f"âœ… AI æ‘˜è¦ç”Ÿæˆå®Œæˆ (æä¾›å•†: {self.ai_provider})", 'success')

                return True, summary

            except Exception as e:
                error_msg = f"âŒ AI æ‘˜è¦ç”Ÿæˆå¤±æ•— (æä¾›å•†: {self.ai_provider}): {str(e)}"
                if log_callback:
                    log_callback(error_msg, 'error')

                # å¦‚æœé‚„æœ‰å˜—è©¦æ©Ÿæœƒï¼Œå˜—è©¦å®¹éŒ¯åˆ‡æ›
                if attempt < max_attempts:
                    if self._try_fallback_provider(log_callback):
                        if log_callback:
                            log_callback(f"ğŸ”„ å˜—è©¦é‡è©¦ï¼Œç•¶å‰æä¾›å•†: {self.ai_provider}", 'info')
                        continue
                    else:
                        if log_callback:
                            log_callback("âŒ æ²’æœ‰å¯ç”¨çš„å‚™ç”¨ AI æä¾›å•†", 'error')
                        break
                else:
                    if log_callback:
                        log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
                    break

        return False, f"âŒ AI æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼Œå·²å˜—è©¦ {attempt} æ¬¡"

    def save_summary(self,
                    summary: str,
                    save_path: Path,
                    log_callback: Optional[Callable] = None) -> bool:
        """
        å„²å­˜æ‘˜è¦åˆ°æª”æ¡ˆ

        Args:
            summary: æ‘˜è¦å…§å®¹
            save_path: å„²å­˜è·¯å¾‘
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸ï¼ˆå¯é¸ï¼‰

        Returns:
            bool: å„²å­˜æˆåŠŸæ¨™èªŒ
        """
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # å¯«å…¥æª”æ¡ˆ
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(summary)

            if log_callback:
                log_callback(f"ğŸ“„ æ‘˜è¦å·²å„²å­˜è‡³: {save_path}", 'info')

            return True

        except Exception as e:
            error_msg = f"âŒ æ‘˜è¦å„²å­˜å¤±æ•—: {str(e)}"
            if log_callback:
                log_callback(error_msg, 'error')
            return False

    def generate_and_save_summary(self,
                                 subtitle_content: str,
                                 save_path: Path,
                                 prompt_type: str = "structured",
                                 header_info: Optional[Dict[str, Any]] = None,
                                 progress_callback: Optional[Callable] = None,
                                 log_callback: Optional[Callable] = None,
                                 telegram_callback: Optional[Callable] = None) -> tuple[bool, str]:
        """
        ç”Ÿæˆä¸¦å„²å­˜æ‘˜è¦ï¼ˆä¸€ç«™å¼æœå‹™ï¼‰

        Args:
            subtitle_content: å­—å¹•å…§å®¹
            save_path: å„²å­˜è·¯å¾‘
            prompt_type: prompté¡å‹
            header_info: headerè³‡è¨Š
            progress_callback: é€²åº¦å›èª¿
            log_callback: æ—¥èªŒå›èª¿
            telegram_callback: Telegramé€šçŸ¥å›èª¿

        Returns:
            tuple: (æˆåŠŸæ¨™èªŒ, æ‘˜è¦å…§å®¹æˆ–éŒ¯èª¤ä¿¡æ¯)
        """
        # ç”Ÿæˆæ‘˜è¦
        success, result = self.generate_summary(
            subtitle_content=subtitle_content,
            prompt_type=prompt_type,
            header_info=header_info,
            progress_callback=progress_callback,
            log_callback=log_callback
        )

        if not success:
            return False, result

        summary = result

        # å„²å­˜æ‘˜è¦
        if not self.save_summary(summary, save_path, log_callback):
            return False, "æ‘˜è¦ç”ŸæˆæˆåŠŸä½†å„²å­˜å¤±æ•—"

        # ç™¼é€æ—¥èªŒ
        if log_callback:
            log_callback(f"---æ‘˜è¦å…§å®¹---\n{summary}", 'info')

        # ç™¼é€Telegramé€šçŸ¥ï¼ˆå¦‚æœæä¾›ï¼‰
        if telegram_callback and header_info:
            try:
                # æå–åŸå§‹æ‘˜è¦ï¼ˆå»é™¤headerï¼‰
                lines = summary.split('\n')
                summary_start = 0
                for i, line in enumerate(lines):
                    if '=' in line and len(line) > 20:  # æ‰¾åˆ°åˆ†éš”ç·š
                        summary_start = i + 2
                        break

                clean_summary = '\n'.join(lines[summary_start:]) if summary_start > 0 else summary

                # æ§‹å»ºTelegramæ¶ˆæ¯
                if 'title' in header_info:
                    # å½±ç‰‡é¡å‹
                    tg_message = (
                        f"âœ… *æ‘˜è¦å®Œæˆ:*\n\n"
                        f"ğŸ¬ *æ¨™é¡Œ:* `{header_info.get('title', 'N/A')}`\n"
                        f"ğŸ“º *é »é“:* `{header_info.get('uploader', 'N/A')}`\n\n"
                        f"ğŸ“ *å®Œæ•´æ‘˜è¦:*\n`{clean_summary[:1000]}{'...' if len(clean_summary) > 1000 else ''}`"
                    )
                else:
                    # æª”æ¡ˆé¡å‹
                    tg_message = (
                        f"âœ… *æ‘˜è¦å®Œæˆ:*\n\n"
                        f"ğŸ“ *æª”æ¡ˆ:* `{header_info.get('filename', 'N/A')}`\n\n"
                        f"ğŸ“ *å®Œæ•´æ‘˜è¦:*\n`{clean_summary[:1000]}{'...' if len(clean_summary) > 1000 else ''}`"
                    )

                telegram_callback(tg_message)

            except Exception as e:
                if log_callback:
                    log_callback(f"âš ï¸ Telegramé€šçŸ¥ç™¼é€å¤±æ•—: {e}", 'warning')

        return True, summary


# å…¨åŸŸæ‘˜è¦æœå‹™å¯¦ä¾‹
_summary_service_instance = None

def get_summary_service(openai_api_key: Optional[str] = None, ai_provider: Optional[str] = None) -> SummaryService:
    """
    ç²å–å…¨åŸŸæ‘˜è¦æœå‹™å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰

    Args:
        openai_api_key: OpenAI API é‡‘é‘°
        ai_provider: AI æä¾›å•†åç¨±

    Returns:
        SummaryService: æ‘˜è¦æœå‹™å¯¦ä¾‹
    """
    global _summary_service_instance

    if _summary_service_instance is None:
        _summary_service_instance = SummaryService(openai_api_key, ai_provider)
    elif openai_api_key:
        # æ›´æ–°APIé‡‘é‘°
        _summary_service_instance.current_provider_config = _summary_service_instance._get_provider_config(ai_provider or _summary_service_instance.ai_provider)
        _summary_service_instance._init_ai_client()

    return _summary_service_instance

def reset_summary_service():
    """é‡ç½®å…¨åŸŸæ‘˜è¦æœå‹™å¯¦ä¾‹"""
    global _summary_service_instance
    _summary_service_instance = None