"""
çµ±ä¸€Whisperæ¨¡å‹ç®¡ç†å™¨ - æ•´åˆæ‰€æœ‰æ¨¡å‹è¼‰å…¥å’Œè½‰éŒ„é‚è¼¯
"""

import os
import platform
import subprocess
import sys
import traceback
from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, Any, Callable

import torch

from src.utils.time_formatter import get_timestamp


@dataclass
class TranscriptSegment:
    """Simple segment container used when third-party libraries return dicts."""
    start: float
    end: float
    text: str


class WhisperModelManager:
    """çµ±ä¸€Whisperæ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, model_name: Optional[str] = None, fallback_model_name: Optional[str] = None):
        detected_backend = self._detect_backend()
        self.preferred_backend = detected_backend
        self.backend = detected_backend
        self._backend_reason: Optional[str] = None

        default_model = model_name or os.getenv("WHISPER_MODEL_NAME") or self._get_default_model(detected_backend)
        default_fallback = fallback_model_name or os.getenv("WHISPER_FALLBACK_MODEL_NAME")
        if not default_fallback:
            default_fallback = "asadfgglie/faster-whisper-large-v3-zh-TW"
        if detected_backend != "mlx":
            # é MLX ç’°å¢ƒä¸‹ï¼Œé è¨­å›é€€æ¨¡å‹èˆ‡ä¸»è¦æ¨¡å‹ä¸€è‡´
            default_fallback = default_model

        self.primary_model_name = default_model
        self.fallback_model_name = default_fallback
        self.model_name = self.primary_model_name
        self.model = None
        self.device = "cpu"
        self.compute_type = "int8"
        self.is_loaded = False
        self._mlx_module = None
        self._mlx_verified = False
        self._mlx_available = False

        if self.backend == "mlx":
            if not self._verify_mlx_backend():
                self._backend_reason = self._backend_reason or "MLX backend verification failed during startup."


    def _detect_backend(self) -> str:
        """Detect the backend to use based on the current platform."""
        system = platform.system().lower()
        machine = platform.machine().lower()

        backend_override = os.getenv("WHISPER_BACKEND")
        if backend_override in {"mlx", "faster_whisper"}:
            return backend_override

        if system == "darwin" and machine in {"arm64", "aarch64"}:
            return "mlx"
        return "faster_whisper"

    def _get_default_model(self, backend: Optional[str] = None) -> str:
        """Return the default model name for the detected backend."""
        backend = backend or self.backend
        if backend == "mlx":
            return "mlx-community/whisper-large-v3-turbo-q4"
        return "asadfgglie/faster-whisper-large-v3-zh-TW"

    def _verify_mlx_backend(self) -> bool:
        """Check whether MLX backend can be safely imported and used."""
        if self._mlx_verified:
            return self._mlx_available

        self._mlx_verified = True

        try:
            result = subprocess.run(
                [sys.executable, "-c", "import mlx_whisper"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
                text=True
            )
            if result.returncode != 0:
                self._backend_reason = (
                    f"MLX backend check failed with code {result.returncode}: {result.stderr.strip()}"
                )
                self._mlx_available = False
                return False

            self._mlx_available = True
            return True
        except Exception as error:
            self._backend_reason = f"MLX backend check raised exception: {error}"
            self._mlx_available = False
            return False

    def _detect_device_and_compute_type(self, prefer_cuda: bool, log_callback: Optional[Callable]) -> Tuple[str, str]:
        device = "cpu"
        compute_type = "int8"

        if prefer_cuda and torch.cuda.is_available():
            try:
                # æ¸¬è©¦ CUDA æ˜¯å¦çœŸçš„å¯ä»¥å·¥ä½œ
                test_tensor = torch.zeros(1, device="cuda")
                del test_tensor

                device = "cuda"
                compute_type = "float16"

                if log_callback:
                    gpu_name = torch.cuda.get_device_name(0)
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    log_callback(f"âœ… CUDA æ¸¬è©¦æˆåŠŸï¼Œä½¿ç”¨ GPU åŠ é€Ÿ", 'success')
                    log_callback(f"ğŸ“± GPU è³‡è¨Š - å‹è™Ÿ: {gpu_name}, é¡¯å­˜: {gpu_memory:.1f}GB", 'info')

            except Exception as cuda_error:
                if log_callback:
                    log_callback(f"âš ï¸ CUDA æ¸¬è©¦å¤±æ•—ï¼š{cuda_error}ï¼Œå›é€€åˆ° CPU", 'warning')

                device = "cpu"
                compute_type = "int8"
        return device, compute_type

    def load_model(self, prefer_cuda: bool = True, log_callback: Optional[Callable] = None) -> bool:
        """
        çµ±ä¸€æ¨¡å‹è¼‰å…¥å‡½æ•¸

        Args:
            prefer_cuda: æ˜¯å¦å„ªå…ˆä½¿ç”¨CUDA
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸

        Returns:
            bool: è¼‰å…¥æ˜¯å¦æˆåŠŸ
        """
        if self.backend == "mlx":
            if not self._verify_mlx_backend():
                if log_callback:
                    reason = self._backend_reason or "MLX backend ç„¡æ³•å•Ÿç”¨"
                    log_callback(f"âŒ {reason}", 'error')
                else:
                    reason = self._backend_reason or "MLX backend ç„¡æ³•å•Ÿç”¨"
                    print(f"[WhisperModelManager] {reason}", flush=True)
                self.is_loaded = False
                return False
            return self._load_mlx_model(log_callback)
        return self._load_faster_whisper_model(prefer_cuda, log_callback)

    def _load_faster_whisper_model(self, prefer_cuda: bool, log_callback: Optional[Callable]) -> bool:
        """Load the model using faster-whisper backend."""
        try:
            import faster_whisper

            if log_callback:
                log_callback(f"ğŸ”„ è¼‰å…¥ Whisper æ¨¡å‹: {self.model_name}...", 'info')

            # é‡ç½®ç‹€æ…‹
            self.model = None
            self.is_loaded = False

            # åµæ¸¬è¨­å‚™å’Œè¨ˆç®—é¡å‹
            self.device, self.compute_type = self._detect_device_and_compute_type(prefer_cuda, log_callback)

            # è¼‰å…¥æ¨¡å‹
            if log_callback:
                log_callback(f"ğŸ”„ è¼‰å…¥æ¨¡å‹ (æ¨¡å‹: {self.model_name}, è¨­å‚™: {self.device}, è¨ˆç®—é¡å‹: {self.compute_type})", 'info')

            self.model = faster_whisper.WhisperModel(
                self.model_name,
                device=self.device,
                compute_type=self.compute_type
            )

            self.is_loaded = True

            if log_callback:
                log_callback(f"âœ… Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ (æ¨¡å‹: {self.model_name}, è¨­å‚™: {self.device})", 'success')

            return True

        except Exception as e:
            error_msg = f"âŒ Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}"
            if log_callback:
                log_callback(error_msg, 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')

            self.is_loaded = False
            return False

    def _load_mlx_model(self, log_callback: Optional[Callable]) -> bool:
        """Load the model using MLX backend on Apple Silicon."""
        try:
            import mlx_whisper  # type: ignore

            self._mlx_module = mlx_whisper

            if log_callback:
                log_callback("ğŸ”„ è¼‰å…¥ MLX Whisper æ¨¡å‹...", 'info')

            # MLX æœƒåœ¨ç¬¬ä¸€æ¬¡åŸ·è¡Œæ™‚è‡ªå‹•ä¸‹è¼‰æ¨¡å‹ï¼Œæˆ‘å€‘åªéœ€ç¢ºèªæ¨¡çµ„å¯ç”¨
            self.model = None  # MLX åœ¨è½‰éŒ„æ™‚å‹•æ…‹è¼‰å…¥
            self.device = "mlx"
            self.compute_type = "int4"
            self.is_loaded = True

            if log_callback:
                log_callback("âœ… MLX Whisper æ¨¡å‹å·²å°±ç·’ (Apple Silicon)", 'success')

            return True

        except ImportError as e:
            if log_callback:
                log_callback(f"âŒ æœªæ‰¾åˆ° MLX Whisper å¥—ä»¶: {e}", 'error')
                log_callback("â„¹ï¸ è«‹å®‰è£ä¾è³´ï¼š pip install mlx-whisper", 'info')
            self._backend_reason = f"MLX package import failed: {e}"
            self.is_loaded = False
            return False

        except Exception as e:
            if log_callback:
                log_callback(f"âŒ MLX Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}", 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
            self._backend_reason = f"MLX model load failed: {e}"
            self.is_loaded = False
            return False

    def transcribe_with_fallback(
        self,
        audio_file: str,
        log_callback: Optional[Callable] = None,
        **transcribe_kwargs
    ) -> Tuple[bool, Optional[List]]:
        """
        å¸¶å›é€€æ©Ÿåˆ¶çš„è½‰éŒ„å‡½æ•¸

        Args:
            audio_file: éŸ³æª”è·¯å¾‘
            log_callback: æ—¥èªŒå›èª¿å‡½æ•¸
            **transcribe_kwargs: è½‰éŒ„åƒæ•¸

        Returns:
            Tuple[bool, Optional[List]]: (æˆåŠŸç‹€æ…‹, ç‰‡æ®µåˆ—è¡¨)
        """
        if not self.is_loaded:
            if log_callback:
                reason_msg = f"âŒ æ¨¡å‹æœªè¼‰å…¥" + (f"ï¼ˆåŸå› : {self._backend_reason})" if self._backend_reason else "")
                log_callback(reason_msg, 'error')
            else:
                reason_msg = "Model not loaded"
                if self._backend_reason:
                    reason_msg += f" (reason: {self._backend_reason})"
                print(f"[WhisperModelManager] {reason_msg}", flush=True)
            return False, None

        # è¨­å®šé è¨­è½‰éŒ„åƒæ•¸
        # RTX3060 12GB: batch_size å»ºè­°å€¼ç‚º 8-16
        # æ ¹æ“šé¡¯å­˜èª¿æ•´ - æ›´å¤§çš„batch_sizeæœƒåŠ å¿«è½‰éŒ„é€Ÿåº¦ä½†æ¶ˆè€—æ›´å¤šGPUè¨˜æ†¶é«”
        default_params = {
            'beam_size': 1,
            'language': "zh",
            'vad_filter': True
#            'batch_size': 12  # RTX3060 12GB é¡¯å­˜æœ€ä½³åŒ–
        }
        default_params.update(transcribe_kwargs)

        if self.backend == "mlx":
            success, segments = self._transcribe_mlx(audio_file, log_callback, **default_params)
            return success, segments

        try:
            if log_callback:
                log_callback("ğŸ¯ é–‹å§‹è½‰éŒ„éŸ³æª”...", 'info')
                log_callback(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {self.model_name}", 'info')
                log_callback(f"âš™ï¸ è½‰éŒ„åƒæ•¸ - batch_size: {default_params.get('batch_size', 1)}, beam_size: {default_params.get('beam_size', 1)}, èªè¨€: {default_params.get('language', 'auto')}", 'info')
                log_callback("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è½‰éŒ„...", 'info')

            # ç¬¬ä¸€æ¬¡å˜—è©¦è½‰éŒ„
            segments, _ = self.model.transcribe(str(audio_file), **default_params)

            if log_callback:
                log_callback("ğŸ”„ è½‰éŒ„é€²è¡Œä¸­ï¼Œæ­£åœ¨è™•ç†ç‰‡æ®µ...", 'info')

            # è½‰æ›ç‚ºåˆ—è¡¨
            segments_list = list(segments)

            if log_callback:
                log_callback(f"âœ… è½‰éŒ„å®Œæˆï¼Œå…± {len(segments_list)} å€‹ç‰‡æ®µ", 'success')

            return True, segments_list

        except RuntimeError as e:
            # æª¢æŸ¥æ˜¯å¦ç‚ºCUDAç›¸é—œéŒ¯èª¤
            if "cublas" in str(e).lower() or "cuda" in str(e).lower():
                if log_callback:
                    log_callback("âš ï¸ CUDA éŒ¯èª¤ï¼Œå˜—è©¦ä½¿ç”¨ CPU é‡æ–°è½‰éŒ„...", 'warning')

                # å˜—è©¦CPUå›é€€
                return self._cpu_fallback_transcribe(audio_file, log_callback, **default_params)
            else:
                if log_callback:
                    log_callback(f"âŒ è½‰éŒ„å¤±æ•—: {e}", 'error')
                    log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
                return False, None

        except Exception as e:
            if log_callback:
                log_callback(f"âŒ è½‰éŒ„å¤±æ•—: {e}", 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
            return False, None

    def _cpu_fallback_transcribe(
        self,
        audio_file: str,
        log_callback: Optional[Callable] = None,
        **transcribe_kwargs
    ) -> Tuple[bool, Optional[List]]:
        """CPUå›é€€è½‰éŒ„"""
        try:
            import faster_whisper

            if log_callback:
                log_callback(f"ğŸ”„ é‡æ–°è¼‰å…¥ CPU æ¨¡å‹: {self.model_name}...", 'info')

            # é‡æ–°è¼‰å…¥CPUæ¨¡å‹
            self.model = faster_whisper.WhisperModel(
                self.model_name,
                device="cpu",
                compute_type="int8"
            )

            self.device = "cpu"
            self.compute_type = "int8"

            # é‡æ–°å˜—è©¦è½‰éŒ„
            if log_callback:
                log_callback(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {self.model_name} (CPU æ¨¡å¼)", 'info')
            segments, _ = self.model.transcribe(str(audio_file), **transcribe_kwargs)

            if log_callback:
                log_callback("ğŸ”„ CPU è½‰éŒ„é€²è¡Œä¸­...", 'info')

            segments_list = list(segments)

            if log_callback:
                log_callback(f"âœ… CPU è½‰éŒ„å®Œæˆï¼Œå…± {len(segments_list)} å€‹ç‰‡æ®µ", 'success')

            return True, segments_list

        except Exception as cpu_error:
            if log_callback:
                log_callback(f"âŒ CPU è½‰éŒ„ä¹Ÿå¤±æ•—: {cpu_error}", 'error')
            return False, None

    def _transcribe_mlx(
        self,
        audio_file: str,
        log_callback: Optional[Callable] = None,
        **transcribe_kwargs
    ) -> Tuple[bool, Optional[List]]:
        """Transcribe audio using the MLX backend."""
        if not self._mlx_module:
            try:
                import mlx_whisper  # type: ignore
                self._mlx_module = mlx_whisper
            except ImportError as import_error:
                if log_callback:
                    log_callback(f"âŒ MLX Whisper å¥—ä»¶æœªå®‰è£: {import_error}", 'error')
                return False, None

        try:
            mlx_whisper = self._mlx_module

            if log_callback:
                log_callback("ğŸ¯ ä½¿ç”¨ MLX é€²è¡Œè½‰éŒ„...", 'info')

            decoding_options = None
            language = transcribe_kwargs.get('language')

            if hasattr(mlx_whisper, "DecodingOptions"):
                options_kwargs: Dict[str, Any] = {"task": "transcribe"}
                if language:
                    options_kwargs["language"] = language
                # beam_size åœ¨ MLX ä¸­å¯é¸ï¼Œè‹¥å­˜åœ¨å‰‡é™„åŠ 
                beam_size = transcribe_kwargs.get('beam_size')
                if beam_size is not None:
                    options_kwargs["beam_size"] = beam_size

                try:
                    decoding_options = mlx_whisper.DecodingOptions(**options_kwargs)
                except TypeError:
                    # è‹¥åƒæ•¸ä¸æ”¯æ´ï¼Œå›é€€åˆ°åŸºç¤é¸é …
                    decoding_options = mlx_whisper.DecodingOptions(task="transcribe")

            result = None
            file_path = str(audio_file)
            transcribe_kwargs: Dict[str, Any] = {"path_or_hf_repo": self.model_name}
            if decoding_options:
                transcribe_kwargs["options"] = decoding_options
            result = mlx_whisper.transcribe(file_path, **transcribe_kwargs)

            segments_raw = []
            if isinstance(result, dict):
                segments_raw = result.get("segments", [])
            elif hasattr(result, "segments"):
                segments_raw = getattr(result, "segments")
            else:
                segments_raw = result

            segments_list: List[TranscriptSegment] = []
            for segment in segments_raw or []:
                if isinstance(segment, dict):
                    start = float(segment.get("start", 0.0))
                    end = float(segment.get("end", start))
                    text = str(segment.get("text", "")).strip()
                else:
                    start = float(getattr(segment, "start", 0.0))
                    end = float(getattr(segment, "end", start))
                    text = str(getattr(segment, "text", "")).strip()
                segments_list.append(TranscriptSegment(start=start, end=end, text=text))

            if log_callback:
                log_callback(f"âœ… MLX è½‰éŒ„å®Œæˆï¼Œå…± {len(segments_list)} å€‹ç‰‡æ®µ", 'success')

            return True, segments_list

        except Exception as mlx_error:
            if log_callback:
                log_callback(f"âŒ MLX è½‰éŒ„å¤±æ•—: {mlx_error}", 'error')
                log_callback(f"ğŸ” éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}", 'error')
            self._backend_reason = f"MLX transcribe failed: {mlx_error}"
            return False, None

    def get_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ç‹€æ…‹"""
        if self.device == 'cuda':
            device_name = torch.cuda.get_device_name(0)
        elif self.device == 'mlx':
            raw_name = platform.machine()
            device_name = f"Apple Silicon ({raw_name})" if raw_name else "Apple Silicon"
        else:
            device_name = 'CPU'

        return {
            'is_loaded': self.is_loaded,
            'device': self.device,
            'device_name': device_name,
            'compute_type': self.compute_type,
            'cuda_available': self.device == 'cuda',
            'mps_available': self.device == 'mlx',
            'backend': self.backend,
            'preferred_backend': self.preferred_backend,
            'model_name': self.model_name,
            'primary_model_name': self.primary_model_name,
            'fallback_model_name': self.fallback_model_name,
            'backend_reason': self._backend_reason,
            'last_updated': get_timestamp()
        }

    def unload_model(self):
        """å¸è¼‰æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”"""
        if self.model:
            del self.model
            self.model = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.is_loaded = False


# å…¨åŸŸæ¨¡å‹ç®¡ç†å™¨å¯¦ä¾‹
whisper_manager = WhisperModelManager()


def get_whisper_manager() -> WhisperModelManager:
    """ç²å–å…¨åŸŸWhisperæ¨¡å‹ç®¡ç†å™¨"""
    return whisper_manager


def transcribe_audio(
    audio_file: str,
    log_callback: Optional[Callable] = None,
    auto_load: bool = True,
    **kwargs
) -> Tuple[bool, Optional[List]]:
    """
    ä¾¿æ·éŸ³æª”è½‰éŒ„å‡½æ•¸

    Args:
        audio_file: éŸ³æª”è·¯å¾‘
        log_callback: æ—¥èªŒå›èª¿å‡½æ•¸
        auto_load: æ˜¯å¦è‡ªå‹•è¼‰å…¥æ¨¡å‹
        **kwargs: å…¶ä»–è½‰éŒ„åƒæ•¸

    Returns:
        Tuple[bool, Optional[List]]: (æˆåŠŸç‹€æ…‹, ç‰‡æ®µåˆ—è¡¨)
    """
    manager = get_whisper_manager()

    # å¦‚æœæ¨¡å‹æœªè¼‰å…¥ä¸”å…è¨±è‡ªå‹•è¼‰å…¥
    if not manager.is_loaded and auto_load:
        if not manager.load_model(log_callback=log_callback):
            return False, None

    return manager.transcribe_with_fallback(audio_file, log_callback, **kwargs)
